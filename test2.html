<!DOCTYPE html>
<html lang="en">

<head>
    <title>HandMemory!!!</title>
    <!-- Be sure to use tfjs 1.7.4, in tfjs 2 they broke their own handpose model-->
    <!-- Require the peer dependencies of handpose. -->
    <script src="https://unpkg.com/@tensorflow/tfjs-core@2.1.0/dist/tf-core.min.js"></script>
    <script src="https://unpkg.com/@tensorflow/tfjs-converter@2.1.0/dist/tf-converter.min.js"></script>

    <!-- You must explicitly require a TF.js backend if you're not using the tfs union bundle. -->
    <script src="https://unpkg.com/@tensorflow/tfjs-backend-webgl@2.1.0/dist/tf-backend-webgl.min.js"></script>
    <!-- Alternatively you can use the WASM backend: <script src="https://unpkg.com/@tensorflow/tfjs-backend-wasm@2.1.0/dist/tf-backend-wasm.js"></script> -->

    <script src="https://unpkg.com/@tensorflow-models/handpose@0.0.6/dist/handpose.min.js"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.10.2/p5.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.10.2/addons/p5.sound.min.js"></script>
    <link rel="stylesheet" type="text/css" href="style.css">
    <meta charset="utf-8" />


    <!-- <input type="file" accept="image/*;capture=camera"> -->
</head>

<body>
    <script>
        /*
 * https://editor.p5js.org/LingDong-/sketches/1viPqbRMv
 */

var handposeModel = null; // this will be loaded with the handpose model
// WARNING: do NOT call it 'model', because p5 already has something called 'model'

var videoDataLoaded = false; // is webcam capture ready?

var statusText = "Loading handpose model...";

var myHands = []; // hands detected by mediapipe
// currently handpose only supports single hand, so this will be either empty or singleton

var capture; // webcam capture, managed by p5.js

// Load the MediaPipe handpose model assets.
handpose.load().then(function (_model) {
	// console.log("model initialized.")
	statusText = "Model loaded."
	handposeModel = _model;
})


function setup() {
	// createCanvas(window.innerWidth, window.innerHeight);
	// createCanvas(640, 360);
	capture = createCapture(VIDEO);

	// this is to make sure the capture is loaded before asking handpose to take a look
	// otherwise handpose will be very unhappy
	capture.elt.onloadeddata = function () {
		// console.log("video initialized");
		videoDataLoaded = true;
		createCanvas(capture.width*0.5, capture.height*0.5);
	}

	capture.hide();

	// textSize(50);
	// strokeWeight(3);

}

// draw a hand object returned by handpose
function drawShape(hands) {

	// Each hand object contains a `landmarks` property,
	// which is an array of 21 3-D landmarks.
	for (var i = 0; i < hands.length; i++) {
		var landmarks = hands[i].landmarks;

		beginShape();
		for (var j = 0; j < landmarks.length; j++) {
			var [x, y, z] = landmarks[j];

			if (j == 4 || j == 8 || j == 12 || j == 16 || j == 20) {
				vertex(x, y);
			}
		}
		endShape(CLOSE);
	}
}


function draw() {


	if (handposeModel && videoDataLoaded) { // model and video both loaded, 

		handposeModel.estimateHands(capture.elt).then(function (_hands) {
			// we're handling an async promise
			// best to avoid drawing something here! it might produce weird results due to racing

			myHands = _hands; // update the global myHands object with the detected hands
			if (!myHands.length) {
				// haven't found any hands
				statusText = "Show some hands!"
			} else {
				// display the confidence, to 3 decimal places
                statusText = "Confidence: " + (Math.round(myHands[0].handInViewConfidence * 1000) / 1000);
                console.log(myHands[0]);
                // console.log(handpose.load());
                
			}

		})
	}

	background(200);

	// first draw the debug video and annotations
	push();
	translate(width, 0);
	scale(-0.5, 0.5); 
	image(capture, 0, 0, width * 2, height * 2);

	// console.log(capture);
	fill(255, 0, 0, 80);
	stroke(255);
	strokeWeight(3);
	drawShape(myHands); // draw my hand skeleton
	pop();

	push();
	fill(255, 0, 0);
	text(statusText, 2, 60);
	pop();
}
    </script>

</body>

</html>